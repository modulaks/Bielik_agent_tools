{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuS1RCUKEa34"
      },
      "outputs": [],
      "source": [
        "!pip -q install -U \\\n",
        "  langgraph langchain langchain-core langchain-community langchain-text-splitters \\ langchain_openai \\ langchain-ollama \\\n",
        "  pypdf chromadb tiktoken python-multipart \\\n",
        "  huggingface-hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RY7SAkBeSESk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "key = os.environ['OPENAI_API_KEY'] = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICtOO0EZEpk6"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFCkv1mpExQR"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugEPngDqE1Gn"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import subprocess\n",
        "import shutil\n",
        "import json\n",
        "import re\n",
        "\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "p = subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
        "time.sleep(8)\n",
        "print(\"Ollama server PID:\", p.pid)\n",
        "\n",
        "OLLAMA_HOST = \"http://localhost:11434\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQzTwpZwE3lC"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "import httpx\n",
        "from pathlib import Path\n",
        "from string import Template\n",
        "from typing import Any, Dict, List, Tuple, Optional\n",
        "\n",
        "MODEL_DIR = Path(\"/content/models/bielik26\")\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "REPO_ID = \"speakleash/Bielik-11B-v2.6-Instruct-GGUF\"\n",
        "GGUF_NAME = \"Bielik-11B-v2.6-Instruct.Q4_K_M.gguf\"\n",
        "\n",
        "gguf_path = hf_hub_download(\n",
        "    repo_id=REPO_ID,\n",
        "    filename=GGUF_NAME,\n",
        "    local_dir=str(MODEL_DIR),\n",
        ")\n",
        "print(\"Downloaded GGUF:\", gguf_path)\n",
        "\n",
        "modelfile_path = MODEL_DIR / \"Bielik-tools.Modelfile\"\n",
        "\n",
        "modelfile_tpl = Template(r'''FROM ./$GGUF\n",
        "\n",
        "TEMPLATE \"\"\"<s>{{- if .System }}<|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "{{ .System }}<|eot_id|>{{- end }}\n",
        "{{- if .Tools }}<|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You have access to the following tools:\n",
        "{{ .Tools }}\n",
        "\n",
        "When you need to call a tool, you MUST respond with ONLY this exact format:\n",
        "<tool_call>\n",
        "{\"name\": \"function_name\", \"arguments\": {\"param\": \"value\"}}\n",
        "</tool_call>\n",
        "\n",
        "Do not add any other text when calling a tool. After receiving tool results, provide your final answer.<|eot_id|>{{- end }}\n",
        "{{- range .Messages }}\n",
        "{{- if eq .Role \"user\" }}<|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{{ .Content }}<|eot_id|>\n",
        "{{- else if eq .Role \"assistant\" }}<|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "{{- if .ToolCalls }}\n",
        "<tool_call>\n",
        "{{- range .ToolCalls }}\n",
        "{\"name\": \"{{ .Function.Name }}\", \"arguments\": {{ .Function.Arguments }}}\n",
        "{{- end }}\n",
        "</tool_call>\n",
        "{{- else }}\n",
        "{{ .Content }}\n",
        "{{- end }}<|eot_id|>\n",
        "{{- else if eq .Role \"tool\" }}<|start_header_id|>tool<|end_header_id|>\n",
        "\n",
        "<tool_response>\n",
        "{{ .Content }}\n",
        "</tool_response><|eot_id|>\n",
        "{{- end }}\n",
        "{{- end }}<|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "PARAMETER stop <|start_header_id|>\n",
        "PARAMETER stop <|end_header_id|>\n",
        "PARAMETER stop <|eot_id|>\n",
        "PARAMETER temperature 0\n",
        "''')\n",
        "\n",
        "modelfile_path.write_text(modelfile_tpl.substitute(GGUF=GGUF_NAME), encoding=\"utf-8\")\n",
        "print(\"Wrote Modelfile:\", str(modelfile_path))\n",
        "\n",
        "!ls -lh /content/models/bielik26 | sed -n '1,60p'\n",
        "\n",
        "# Create model\n",
        "!cd /content/models/bielik26 && ollama create bielik-tools -f Bielik-tools.Modelfile\n",
        "\n",
        "!ollama list\n",
        "!ollama show bielik-tools | sed -n '1,220p'\n",
        "\n",
        "model = \"bielik-tools\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXpn8zqxE-EI"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.tools import tool\n",
        "\n",
        "emb = OpenAIEmbeddings()\n",
        "\n",
        "BASE = \"/content/drive/MyDrive\"\n",
        "\n",
        "CHROMA_BASE = \"/content/chroma_fado\"\n",
        "\n",
        "REBUILD_INDEX = True\n",
        "if REBUILD_INDEX:\n",
        "    shutil.rmtree(CHROMA_BASE, ignore_errors=True)\n",
        "os.makedirs(CHROMA_BASE, exist_ok=True)\n",
        "\n",
        "def _build_retriever(\n",
        "    pdf_path: str,\n",
        "    collection_name: str,\n",
        "    persist_dir: str,\n",
        "    chunk_size: int = 1000,\n",
        "    chunk_overlap: int = 40,\n",
        "    k: int = 4,\n",
        "    lambda_mult: float = 0.5,\n",
        "    fetch_k: int = 10,\n",
        "):\n",
        "    loader = PyPDFLoader(pdf_path, extraction_mode=\"layout\", extract_images=False)\n",
        "    data = loader.load()\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    docs = splitter.split_documents(data)\n",
        "\n",
        "    vs = Chroma.from_documents(\n",
        "        documents=docs,\n",
        "        embedding=emb,\n",
        "        collection_name=collection_name,\n",
        "        persist_directory=persist_dir,\n",
        "    )\n",
        "    return vs.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={\"k\": k, \"lambda_mult\": lambda_mult, \"fetch_k\": fetch_k},\n",
        "    )\n",
        "\n",
        "general_retriever = _build_retriever(\n",
        "    f\"{BASE}/Case_PRO_1.pdf\",\n",
        "    \"fado_general\",\n",
        "    f\"{CHROMA_BASE}/general\",\n",
        "    chunk_size=1000, chunk_overlap=40,\n",
        "    k=2, lambda_mult=0.5, fetch_k=5,\n",
        ")\n",
        "\n",
        "operational_retriever = _build_retriever(\n",
        "    f\"{BASE}/Case_PRO_1.pdf\",\n",
        "    \"fado_operational\",\n",
        "    f\"{CHROMA_BASE}/operational\",\n",
        "    chunk_size=1000, chunk_overlap=40,\n",
        "    k=2, lambda_mult=0.5, fetch_k=5,\n",
        ")\n",
        "\n",
        "financial_retriever = _build_retriever(\n",
        "    f\"{BASE}/9_Baby_AGI/Dane_finansowe.pdf\",\n",
        "    \"fado_financial\",\n",
        "    f\"{CHROMA_BASE}/financial\",\n",
        "    chunk_size=1000, chunk_overlap=40,\n",
        "    k=5, lambda_mult=0.0, fetch_k=5,\n",
        ")\n",
        "\n",
        "marketing_retriever = _build_retriever(\n",
        "    f\"{BASE}/9_Baby_AGI/Dane_sprzedażowe_i_marketingowe.pdf\",\n",
        "    \"fado_marketing\",\n",
        "    f\"{CHROMA_BASE}/marketing\",\n",
        "    chunk_size=1000, chunk_overlap=100,\n",
        "    k=5, lambda_mult=0.5, fetch_k=10,\n",
        ")\n",
        "\n",
        "@tool(\"general_retriever\")\n",
        "def general_retriever_tool(query: str) -> str:\n",
        "    \"\"\"Use this tool whenever someone asks for FADO in general.\"\"\"\n",
        "    docs = general_retriever.invoke(query)\n",
        "    return \"\\n\\n\".join(\n",
        "        f\"[{d.metadata.get('source')} | p{d.metadata.get('page')}]\\n{d.page_content}\"\n",
        "        for d in docs\n",
        "    )\n",
        "\n",
        "@tool(\"operational_retriever\")\n",
        "def operational_retriever_tool(query: str) -> str:\n",
        "    \"\"\"Use this tool whenever someone asks for FADO business processes.\"\"\"\n",
        "    docs = operational_retriever.invoke(query)\n",
        "    return \"\\n\\n\".join(\n",
        "        f\"[{d.metadata.get('source')} | p{d.metadata.get('page')}]\\n{d.page_content}\"\n",
        "        for d in docs\n",
        "    )\n",
        "\n",
        "@tool(\"financial_retriever\")\n",
        "def financial_retriever_tool(query: str) -> str:\n",
        "    \"\"\"Use this tool whenever someone asks for FADO financial situation (including statements).\"\"\"\n",
        "    docs = financial_retriever.invoke(query)\n",
        "    return \"\\n\\n\".join(\n",
        "        f\"[{d.metadata.get('source')} | p{d.metadata.get('page')}]\\n{d.page_content}\"\n",
        "        for d in docs\n",
        "    )\n",
        "\n",
        "@tool(\"marketing_retriever\")\n",
        "def marketing_retriever_tool(query: str) -> str:\n",
        "    \"\"\"Use this tool whenever someone asks for FADO products, sales, marketing and margins.\"\"\"\n",
        "    docs = marketing_retriever.invoke(query)\n",
        "    return \"\\n\\n\".join(\n",
        "        f\"[{d.metadata.get('source')} | p{d.metadata.get('page')}]\\n{d.page_content}\"\n",
        "        for d in docs\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6u4eNS0wK9N7"
      },
      "outputs": [],
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "from langchain.agents import create_agent\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "OLLAMA_HOST = \"http://localhost:11434\"\n",
        "MODEL_NAME = \"bielik-tools\"\n",
        "\n",
        "llm = ChatOllama(\n",
        "    model=MODEL_NAME,\n",
        "    base_url=OLLAMA_HOST,\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"Jesteś asystentem pracującym na dokumentach firmy FADO.\n",
        "\n",
        "Cel:\n",
        "- Odpowiadaj po polsku, rzeczowo i na temat.\n",
        "- Opieraj się na informacjach z narzędzi (retrieverów) i treści rozmowy.\n",
        "- Jeśli brakuje danych w kontekście: NAJPIERW użyj narzędzia, dopiero potem odpowiadaj.\n",
        "\n",
        "Dobór narzędzia:\n",
        "- general_retriever: ogólne informacje o FADO / przekrojowe pytania\n",
        "- operational_retriever: procesy, operacje, sposób działania\n",
        "- financial_retriever: finanse, wyniki, wskaźniki, rachunki\n",
        "- marketing_retriever: produkty, sprzedaż, marketing, marże\n",
        "\n",
        "Zasady jakości:\n",
        "- Nie zgaduj liczb ani faktów. Jeśli nie ma ich w wynikach narzędzi, powiedz wprost czego brakuje.\n",
        "- Gdy wyniki są niejednoznaczne, podaj 2–3 możliwe interpretacje i wskaż co trzeba doprecyzować.\n",
        "- Preferuj krótkie punkty + krótkie podsumowanie.\n",
        "- Nie ujawniaj rozumowania krok-po-kroku ani “przemyśleń”. Pokaż tylko wynik.\n",
        "\n",
        "Źródła:\n",
        "- Zawsze podwaj źródło informacji na podstwiw którego formuujesz odpowidz (1. odpowiedzi na bazie contekstu narzędza lub 2. odwpowiedzna bazie wiedzy moelu) !!!\n",
        "\n",
        "Proces przygotowania odpowiedzi (WEWNĘTRZNY – nie wypisuj kroków):\n",
        "1) Wygeneruj wstępny szkic odpowiedzi.\n",
        "2) Zrób krótką refleksję: czy odpowiedź jest kompletna i czy każde zdanie wynika z wyników narzędzi lub kontekstu rozmowy?\n",
        "   - Jeśli coś jest domysłem albo nie wynika z danych: usuń to lub jasno powiedz „brak danych”.\n",
        "3) Jeśli można poprawić precyzję: doprecyzuj, ale wyłącznie na podstawie wyników narzędzi i kontekstu rozmowy.\n",
        "4) Zredaguj odpowiedź końcową.\n",
        "5) Wypisz WYŁĄCZNIE odpowiedź końcową (bez kroków, bez metakomentarzy, bez „myślenia na głos”).\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "checkpointer = InMemorySaver()\n",
        "\n",
        "tools = [\n",
        "    general_retriever_tool,\n",
        "    operational_retriever_tool,\n",
        "    financial_retriever_tool,\n",
        "    marketing_retriever_tool,\n",
        "]\n",
        "\n",
        "agent = create_agent(\n",
        "    model=llm,\n",
        "    tools=tools,\n",
        "    system_prompt=SYSTEM_PROMPT,\n",
        "    checkpointer=checkpointer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGybSPNoLH5E"
      },
      "outputs": [],
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"fado-session-1\"}}\n",
        "\n",
        "result = agent.invoke({\"messages\": [(\"user\", \"Jak działa FADO?\")]}, config)\n",
        "print(result[\"messages\"])\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0xuneiuQqax"
      },
      "outputs": [],
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"fado-session-2\"}}\n",
        "\n",
        "r1 = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Na jakich rynkach działa FADO?\"}]}, config)\n",
        "print(r1[\"messages\"][-1].content)\n",
        "\n",
        "r2 = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Przyponij o co pyatłem poprzednio?\"}]}, config)\n",
        "print(r2[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMP1FCeiOIbh"
      },
      "outputs": [],
      "source": [
        "from typing import List, Any\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage\n",
        "\n",
        "class ProcessGroup(BaseModel):\n",
        "    kategoria: str = Field(description=\"Np. Sprzedaż, Operacje, Obsługa klienta, Finanse, IT\")\n",
        "    procesy: List[str] = Field(description=\"Lista procesów w tej kategorii\")\n",
        "\n",
        "class FadoProcesses(BaseModel):\n",
        "    grupy: List[ProcessGroup]\n",
        "    braki_danych: List[str] = Field(default_factory=list, description=\"Co jest nieznane / czego brakuje w dokumentach\")\n",
        "\n",
        "tools = [\n",
        "    general_retriever_tool,\n",
        "    operational_retriever_tool,\n",
        "    financial_retriever_tool,\n",
        "    marketing_retriever_tool,\n",
        "]\n",
        "\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "tool_map = {getattr(t, \"name\", t.__class__.__name__): t for t in tools}\n",
        "\n",
        "def _stringify_tool_output(out: Any) -> str:\n",
        "    \"\"\"Zamienia różne typy zwrotek (Documents/list/dict/str) na czytelny tekst.\"\"\"\n",
        "    if out is None:\n",
        "        return \"\"\n",
        "    if isinstance(out, list):\n",
        "        parts = []\n",
        "        for x in out:\n",
        "            page = getattr(x, \"page_content\", None)\n",
        "            if page is not None:\n",
        "                parts.append(str(page))\n",
        "            else:\n",
        "                parts.append(str(x))\n",
        "        return \"\\n\\n\".join([p for p in parts if p.strip()]).strip()\n",
        "    return str(out).strip()\n",
        "\n",
        "EXTRACTION_SYSTEM = \"\"\"Jesteś asystentem pracującym na dokumentach firmy FADO.\n",
        "\n",
        "Zasady:\n",
        "- Jeśli brakuje danych w rozmowie, użyj narzędzi (retrieverów).\n",
        "- Wykonaj ekstrakcję informacji o procesach biznesowych FADO.\n",
        "- NIE twórz listy procesów z wiedzy ogólnej. Jeśli czegoś nie ma w wynikach narzędzi, to tego nie dopisuj.\n",
        "- Możesz użyć wielu narzędzi, jeśli potrzeba.\n",
        "- Na tym etapie Twoim celem jest ZEBRANIE materiału z narzędzi (tool calls), nie format końcowy.\n",
        "\"\"\"\n",
        "\n",
        "EXTRACTION_USER = (\n",
        "    \"Zbierz z dokumentów FADO informacje o procesach biznesowych i ich podziale na kategorie \"\n",
        "    \"(np. Sprzedaż, Operacje, Obsługa klienta, Finanse, IT). \"\n",
        "    \"Jeśli nie masz danych w kontekście rozmowy, wywołaj odpowiednie narzędzia.\"\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=EXTRACTION_SYSTEM),\n",
        "    HumanMessage(content=EXTRACTION_USER),\n",
        "]\n",
        "\n",
        "for _ in range(3):\n",
        "    ai = llm_with_tools.invoke(messages)\n",
        "    messages.append(ai)\n",
        "\n",
        "    tool_calls = getattr(ai, \"tool_calls\", None) or []\n",
        "    if not tool_calls:\n",
        "        break\n",
        "\n",
        "    for tc in tool_calls:\n",
        "        name = tc.get(\"name\")\n",
        "        args = tc.get(\"args\") or tc.get(\"arguments\") or {}\n",
        "        tc_id = tc.get(\"id\")\n",
        "\n",
        "        tool = tool_map.get(name)\n",
        "        if tool is None:\n",
        "            messages.append(\n",
        "                ToolMessage(content=f\"Nieznane narzędzie: {name}\", tool_call_id=tc_id)\n",
        "            )\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            out = tool.invoke(args)\n",
        "            out_text = _stringify_tool_output(out)\n",
        "        except Exception as e:\n",
        "            out_text = f\"Błąd wywołania narzędzia {name}: {e}\"\n",
        "\n",
        "        messages.append(ToolMessage(content=out_text, tool_call_id=tc_id))\n",
        "\n",
        "\n",
        "tool_context = \"\\n\\n\".join(\n",
        "    [m.content for m in messages if isinstance(m, ToolMessage) and m.content.strip()]\n",
        ").strip()\n",
        "print(\"DEBUG - content z narzędzia\", tool_context)\n",
        "\n",
        "structured_llm = llm.with_structured_output(\n",
        "    FadoProcesses,\n",
        "    method=\"json_schema\",\n",
        "    include_raw=True,\n",
        ")\n",
        "\n",
        "STRUCTURE_SYSTEM = \"\"\"Jesteś asystentem pracującym na dokumentach firmy FADO.\n",
        "\n",
        "Zasady (twarde):\n",
        "- Używaj WYŁĄCZNIE informacji z KONTEKSTU (wyniki narzędzi) oraz treści pytania.\n",
        "- Nie dopowiadaj procesów ani faktów z wiedzy ogólnej.\n",
        "- Jeśli w kontekście brakuje danych do pogrupowania procesów, wpisz to do braki_danych.\n",
        "- Zwróć WYŁĄCZNIE JSON zgodny ze schematem (bez komentarzy).\n",
        "\"\"\"\n",
        "\n",
        "STRUCTURE_USER = f\"\"\"Pogrupuj procesy biznesowe FADO na kategorie i zwróć wynik zgodny ze schematem.\n",
        "Jeśli nie masz danych w kontekście, wpisz je w braki_danych.\n",
        "\n",
        "KONTEKST (wyniki narzędzi):\n",
        "{tool_context if tool_context else \"BRAK WYNIKÓW Z NARZĘDZI – nie ma danych do ekstrakcji.\"}\n",
        "\"\"\"\n",
        "\n",
        "response = structured_llm.invoke([\n",
        "    SystemMessage(content=STRUCTURE_SYSTEM),\n",
        "    HumanMessage(content=STRUCTURE_USER),\n",
        "])\n",
        "\n",
        "print(\"\\nPARSED:\\n\", response[\"parsed\"])\n",
        "print(\"\\nRAW:\\n\", response[\"raw\"].content)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}